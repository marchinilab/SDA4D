---
title: "Introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here we provide a basic introduction to the SDA4D package, an implementation of the Bayesian sparse tensor decomposition as described in brief below.  We will first explain how to generate a 4D tensor, and run a simple example, and then describe the model in some detail in order to describe the output fully.  The SDA4D package has minimal requirements; following installation no other packages should be required.

## Generating a data tensor

First, ensure you've loaded the package.
```{r setup}
# #Load the library SDA4D
# #requires devtools
#install.packages('devtools')
#devtools::install_github('https://github.com/marchinilab/SDA4D')
# #load library
library(SDA4D)
```

We provide a function to generate a data tensor similar to those we used in simulations, named `generate_example_data` which also returns a list of the matrices used to generate it.

```{r generatedata}
set.seed(42)
generatedData <- generate_example_data()
 
lapply(generatedData,dim)
```

## Performing a short run of the tensor decomposition

Now, setting `stopping=FALSE` to ignore the stopping criterion we run the tensor decomposition with the function `RunSDA4D` for 100 iterations as follows:

```{r runmethod,results=FALSE}
 res<-RunSDA4D(data_tensor = generatedData$dataTensor,
               dimn_vector = c(200,500,16,3),
               max_iters = 100,
               num_components = 8,
               stopping=FALSE)
```
Note that we have suppressed the console output from this run in the file here.

In the output we have the evidence lower bound (ELBO, also sometimes referred to as the negative free energy), recorded at each iteration, the maximum number of iterations, and a number of other parameters.  Note that in the code we have omitted constant terms from the ELBO.

```{r plotELBO,fig.width=8,fig.height=6,echo=FALSE}
#res<-list(Neg_FE=exp(rnorm(100)))
 # plot(-log(-res$Neg_FE[-1]),col='red',pch=4,xlab='Iteration',main='log(ELBO) over 100 iterations')
 # lines(-log(-res$Neg_FE[-1]),col='red')
```

```{r plotELBOqplot,echo=FALSE,fig.width=6,fig.height=4}
library(ggplot2)
startfrom=5
ELBO_plot<-qplot(x=c(startfrom:length(res$ELBO)),
                 y=res$ELBO[-c(1:(startfrom-1))],
                 geom=c("point", "line"))+
            ylab('ELBO')+
            xlab('Iteration')+
            ggtitle('ELBO over 100 iterations')
suppressWarnings(print(ELBO_plot))
```


## The model and main function output

The output of the main function is a list of a number of components:

The main RunSDA4D provides an interface to an implementation of a Bayesian sparse parallel factor analysis model for four-dimensional tensor decomposition.  The output therefore consists of a list of the approximate (variational) posterior distributions of the main parameters, in the form of their parameters, or point estimates where these were calculated instead.  

```{r outputnames}
names(res)
res$maximumiteration
length(res$ELBO)
```

We accept a data tensor $Y \in \mathbb{R}^{N\times L \times M\times T}$ and model this as 
$$y_{nlmt} = \sum_{c=1}^C a_{nc}b_{tc}d_{mc}x_{cl} +\epsilon_{nlmt},$$
where $\epsilon_{nlmt} \sim N(0,\lambda_{lt}^{-1})$ is Gaussian noise, and $c$ indexes the $C$ components.  The model was designed with gene expression data in mind, with $N,L,M,T$ representing the number of samples, genes,time points, and tissues resprectively.  

We place standard Gaussian priors on $a_{nc},b_{tc},d_{mc}$, and a spike and slab sparsity inducing prior on $x_{cl}$, rewriting $x_{cl}=w_{cl}s_{cl}$ where $w_{cl}$ is the diffuse Guassian 'slab' $N(0,\beta_c^{-1})$ and $s_{cl}$ is the Bernoulli 'spike' at 0 with probability $p_{cl}$.  We describe the model in full below.

We apply a structured mean field variational Bayes approach,  pairing $w_{cl},s_{cl}$, but partitioning the remaining latent variables completely.  In the analytical updates we obtain Gaussian posteriors on the $a_{nc}, b_{tc},d_{mc}$, and we record in matrices the sets of means, precisions and second moments in the output.  

The analytic updates of $W,S$ yield a spike and slab posterior with probability $\gamma_{cl}$ for the Bernoulli variable $s_{cl}$, and mean $m$, precision $\sigma$ for the slab $w_{cl}$.  We also record the first and second moment of the product $w_{cl}s_{cl}$.

```{r outputnamesanddims}
lapply(res$A,dim)

lapply(res$WS,dim)

identical(res$WS$mom1,res$WS$m*res$WS$gamma)
```

We summarise the model as follows for a data tensor $Y = (y_{nlmt})_{n,l,m,t}$:

### The Likelihood

$$
\begin{aligned}
\mathbb{P}(Y\vert\Theta) &= \prod_{n,l,m,t}N\left(y_{nlmt} \lvert \sum_{c=1}^C a_{nc}b_{tc}d_{mc}w_{cl}s_{cl},\lambda_{lt}^{-1}\right). 
\end{aligned}
$$

### The Prior

$$
\begin{aligned}
\mathbb{P}(\lambda_{lt}) &= \mbox{Gamma}(\lambda_{lt}\vert u,v);\\
\mathbb{P}(a_{nc}) &= N(a_{nc}\vert 0,1);\\
\mathbb{P}(b_{tc}) &= N(b_{tc}\vert 0,1);\\
\mathbb{P}(d_{mc}) &= N(d_{mc}\vert 0,1);\\
\mathbb{P}(w_{cl}\vert \beta_c) &= N(w_{cl}\vert 0,\beta_c^{-1});\\
\mathbb{P}(s_{cl}\vert \phi_{cl},\psi_{cl}) &= \mbox{Beta}(s_{cl}\vert \psi_{cl}\phi_{cl});\\
\mathbb{P}(\beta_c) &= \mbox{Gamma}(\beta_c\vert e,f);\\
\mathbb{P}(\phi_{cl}\vert\rho_c) &= \mbox{Bernoulli}(\phi_{cl}\vert \rho_c);\\
\mathbb{P}(\psi_{cl}) &= \mbox{Beta}(\psi_{cl}\vert g,h);\\
\mathbb{P}(\rho_c) &= \mbox{Beta}(\rho_c\vert r,z). 
\end{aligned}
$$
In deriving our Variational inference algorithm, we partition the variables as $$\{a_{nc}\},\{b_{tc}\},\{d_{mc}\},\{\phi_{cl}\},\{\psi_{cl}\},\{\rho_c\},\{\beta_c\},\{w_{cl},s_{cl}\}$$

The variables $a_{nc},b_{tc},d_{mc},w_{cl},s_{cl},\beta_c,$ all have analytical updates and so their estimates are reported as lists of matrices (or vectors) of parameters and moments. The remaining variables $\phi_{cl},\psi_{cl},\rho_c$ are found by optimizing the corresponding terms of the ELBO, and have point estimates reported.
